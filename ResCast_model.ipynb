{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZwcfF2Sz4MN"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T16:28:58.761605Z",
     "iopub.status.busy": "2021-07-26T16:28:58.761354Z",
     "iopub.status.idle": "2021-07-26T16:29:26.926981Z",
     "shell.execute_reply": "2021-07-26T16:29:26.926035Z",
     "shell.execute_reply.started": "2021-07-26T16:28:58.761540Z"
    },
    "id": "7slHLnO7FIAR",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima\n",
    "!pip install hydroeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T16:29:56.099897Z",
     "iopub.status.busy": "2021-07-26T16:29:56.099676Z",
     "iopub.status.idle": "2021-07-26T16:29:56.120158Z",
     "shell.execute_reply": "2021-07-26T16:29:56.119071Z",
     "shell.execute_reply.started": "2021-07-26T16:29:56.099874Z"
    },
    "id": "83MeWIruzzyg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hydroeval as he\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzYWirCu0gZr"
   },
   "source": [
    "# Define Dataset Importer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b78DLnue3Oq7"
   },
   "outputs": [],
   "source": [
    "class DatasetImporter:\n",
    "  def __init__(self, csv_path, start_date, end_date):\n",
    "    self.csv_path = csv_path\n",
    "    self.start_date = start_date\n",
    "    self.end_date = end_date\n",
    "  \n",
    "  # TODO: change default for endog_col_name\n",
    "  def import_data(self, endog_col_name='krs_level_m', date_col_name='date'):\n",
    "    self.dataset = read_csv(self.csv_path, header=0, dayfirst=True, parse_dates=[date_col_name], index_col=[date_col_name])\n",
    "    # Construct arrays for endogenous / dependent variables\n",
    "    self.df_endog = self.dataset.filter([endog_col_name])\n",
    "    # Filter data from start_date to end_date\n",
    "    self.df_endog = self.df_endog[self.start_date:self.end_date]\n",
    "    # Get array from dataframe\n",
    "    self.arr_endog = self.df_endog.values\n",
    "\n",
    "    # Construct arrays for exogenous / independent variables\n",
    "    # Drop the endog_col_name column\n",
    "    self.df_exog = self.dataset.drop([endog_col_name], axis=1)\n",
    "    # Filter data from start_date to end_date\n",
    "    self.df_exog = self.df_exog[self.start_date:self.end_date]\n",
    "    # Get array from dataframe\n",
    "    self.arr_exog = self.df_exog.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cylSQ9kDLeXB"
   },
   "source": [
    "# Data Transformation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajxgobSnLiMN"
   },
   "outputs": [],
   "source": [
    "# splits given data into train and test depending on the train_size\n",
    "def train_test_split(data, train_size):\n",
    "  train, test = data[:train_size], data[train_size:]\n",
    "  return train, test\n",
    "\n",
    "# converts given data into a step sequence of given step\n",
    "def step_sequence_converter(data, step):\n",
    "  step_seq = list()\n",
    "  for i in range(len(data) - step):\n",
    "    step_seq.append(data[i:i+step])\n",
    "  return np.array(step_seq)\n",
    "\n",
    "# split a univariate data into samples\n",
    "def split_sequence(data, n_steps_in, n_steps_out):\n",
    "\tx, y = list(), list()\n",
    "\tfor i in range(len(data)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(data):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tdata_x, data_y = data[i:end_ix], data[end_ix:out_end_ix]\n",
    "\t\tx.append(data_x)\n",
    "\t\ty.append(data_y)\n",
    "\treturn np.array(x), np.array(y)\n",
    "\n",
    "# calculate test residuals by subtracting predicted values from actual values\n",
    "def calculate_test_residuals(actuals, predictions):\n",
    "  test_residuals = list()\n",
    "  for i in range(len(predictions)):\n",
    "    test_residuals.append(actuals[i] - predictions[i])\n",
    "  return np.array(test_residuals)\n",
    "\n",
    "# get column from array of tuples\n",
    "def column_from_array_of_tuples(arr, column_no):\n",
    "  return [elem[column_no] for elem in arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJMnvtyRjzh3"
   },
   "source": [
    "# Saving and Loading model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IeZchk5j13c"
   },
   "outputs": [],
   "source": [
    "# saves sarimax model using pickle\n",
    "def save_pickle_model(model, filename):\n",
    "  pickle.dump(model, open(f'{model_data_folder_path}/{filename}.sav', 'wb'))\n",
    "  print(f'Model saved successfully => {model_data_folder_path}/{filename}.sav')\n",
    "\n",
    "# loads sarimax model using pickle\n",
    "def load_pickle_model(filepath):\n",
    "  loaded_model = pickle.load(open(filepath, 'rb'))\n",
    "  print(f'Model loaded successfully')\n",
    "  return loaded_model\n",
    "\n",
    "# saves MLP model\n",
    "def save_NN_model(model, model_name):\n",
    "  model.save(f'{model_data_folder_path}/{model_name}')\n",
    "  print(f'MLP Model saved successfully => {model_data_folder_path}/{model_name}')\n",
    "\n",
    "# Load saved MLP model\n",
    "def load_NN_model(model_name):\n",
    "  return keras.models.load_model(f'{model_data_folder_path}/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZIXQ1Grblp5"
   },
   "source": [
    "# Saving and Loading predictions methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcrXFbscbk7m"
   },
   "outputs": [],
   "source": [
    "# saves predictions array to csv\n",
    "def save_predictions(predictions, filename):\n",
    "  pd.DataFrame(predictions).to_csv(f'{model_data_folder_path}/{filename}.csv')\n",
    "  print(f'Predictions saved successfully => {model_data_folder_path}/{filename}.csv')\n",
    "\n",
    "# loads predictions from csv\n",
    "def load_predictions(filepath):\n",
    "  return read_csv(filepath,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKm6SzAxZj7w"
   },
   "source": [
    "# Error & Efficiency Calculation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SderI6DHaZmx"
   },
   "outputs": [],
   "source": [
    "# calculates root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "  return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# calculates nash sutcliffe efficiency or nse\n",
    "def measure_nse(actual, predicted):\n",
    "  nse = he.evaluator(he.nse, predicted, actual)\n",
    "  return nse[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avf7-qI0jtoZ"
   },
   "source": [
    "# Sarimax Model Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMI1UKUo9i8d"
   },
   "source": [
    "## Sarimax Training & Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FdfjDF69o7j"
   },
   "outputs": [],
   "source": [
    "# one-step sarima forecast\n",
    "def sarimax_forecast(model_fit, exog_forecast, steps):\n",
    "  # make steps prediction and return the predictions\n",
    "  prediction = model_fit.forecast(exog=exog_forecast, steps=steps)\n",
    "  return prediction\n",
    "\n",
    "# one-step sarima forecast\n",
    "def sarimax_train_and_forecast(endog_train, exog_train, exog_forecast, order, seasonal_order, steps):\n",
    "  # define model\n",
    "  model = SARIMAX(endog_train, exog=exog_train, order=order, seasonal_order=seasonal_order)\n",
    "  # fit model\n",
    "  model_fit = model.fit()\n",
    "  # make one step forecast which would give n_steps forecasts\n",
    "  prediction = model_fit.forecast(exog=exog_forecast, steps=steps)\n",
    "  return prediction, model_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RICYzqcH30xp"
   },
   "source": [
    "## Sarimax Weekly Walk Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryrL2b76hFBn"
   },
   "outputs": [],
   "source": [
    "# walk-forward validation for arima\n",
    "def sarimax_weekly_walk_forward_validation(endog, exog, order, seasonal_order, steps, train_size):\n",
    "  model_fit=None\n",
    "  predictions = list()\n",
    "  # split dataset\n",
    "  endog_train, endog_test = train_test_split(endog, train_size)\n",
    "  exog_train, exog_test = train_test_split(exog, train_size)\n",
    "  # seed history with training dataset\n",
    "  history_endog = [x for x in endog_train]\n",
    "  history_exog = [x for x in exog_train]\n",
    "  # step over each time-step in the test set\n",
    "  for i in range(len(endog_test) - steps):\n",
    "      if i % 7 == 0:\n",
    "        # retrain model on history_endog and forecast for next steps\n",
    "        # fit model and make forecast for history\n",
    "        (prediction, model_fit) = sarimax_train_and_forecast(\n",
    "            endog_train=history_endog,\n",
    "            exog_train=history_exog,\n",
    "            exog_forecast=exog_test[i:i + steps],\n",
    "            order=order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            steps=steps,\n",
    "            )\n",
    "        # print(f\"RetrainAndPredictionRun {i}/{len(endog_test)} ===> prediction = {prediction}\")\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(prediction)\n",
    "        # add actual observation to history for the next loop\n",
    "        history_endog.append(endog_test[i])\n",
    "        history_exog.append(exog_test[i])\n",
    "      else:\n",
    "        # do not retrain model. Just make n_steps_out + (i%7) forecasts\n",
    "        prediction = sarimax_forecast(\n",
    "            model_fit=model_fit, \n",
    "            exog_forecast=exog_test[int(i/7)*7:i + steps],\n",
    "            steps=(steps + (i%7))\n",
    "            )\n",
    "        # store last (steps) forecasts to the predictions array\n",
    "        predictions.append(prediction[-steps:])\n",
    "        # add actual observation to history for the next loop\n",
    "        history_endog.append(endog_test[i])\n",
    "        history_exog.append(exog_test[i])\n",
    "      print(f'Run {i}/{len(endog_test) - steps} complete')\n",
    "  # estimate prediction error\n",
    "  rmse = measure_rmse(step_sequence_converter(endog_test.reshape(len(endog_test,)), steps), predictions)\n",
    "  nse = measure_nse(step_sequence_converter(endog_test.reshape(len(endog_test,)), steps).flatten(), np.array(predictions).flatten())\n",
    "  return model_fit, predictions, rmse, nse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baECVJcrp9gr"
   },
   "source": [
    "# MLP Model Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDFqLW4j-caA"
   },
   "source": [
    "## MLP Training and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aQMBFiB-f7z"
   },
   "outputs": [],
   "source": [
    "# one-step MLP forecast\n",
    "def mlp_train_and_forecast(train_x, train_y, test_x, n_steps_in, n_steps_out, dense_units, epochs):\n",
    "  # define model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(dense_units, activation='relu', input_dim=n_steps_in))\n",
    "  model.add(Dense(n_steps_out))\n",
    "  model.compile(optimizer='adam', loss='mse')\n",
    "  # fit model\n",
    "  model.fit(train_x, train_y, epochs=epochs, verbose=0)\n",
    "  prediction = model.predict(test_x, verbose=0)\n",
    "  return prediction, model\n",
    "\n",
    "def mlp_forecast(model, test_x):\n",
    "  prediction = model.predict(test_x, verbose=0)\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1sJGXJGrDE8"
   },
   "source": [
    "## MLP Weekly Walk Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bquRupdurGy1"
   },
   "outputs": [],
   "source": [
    "# walk-forward validation for MLP\n",
    "def mlp_weekly_walk_forward_validation(train, test, n_steps_in, n_steps_out, dense_units=100, epochs=100):\n",
    "  model_fit=None\n",
    "  predictions = list()\n",
    "  # split data into samples\n",
    "  train_x, train_y = split_sequence(train, n_steps_in, n_steps_out)\n",
    "  test_x, test_y = split_sequence(test, n_steps_in, n_steps_out)\n",
    "  # seed history with training dataset\n",
    "  history = train_x\n",
    "  # step over each time-step in the test set\n",
    "  for i in range(len(test_x)):\n",
    "      print(f\"Run {i+1}/{len(test_x)}\")\n",
    "      if i%7 == 0:\n",
    "        # fit model and make forecast for history\n",
    "        (prediction, model_fit) = mlp_train_and_forecast(\n",
    "            train_x=train_x,\n",
    "            train_y=train_y,\n",
    "            test_x=test_x[i].reshape((1, n_steps_in)),\n",
    "            n_steps_in=n_steps_in,\n",
    "            n_steps_out=n_steps_out,\n",
    "            dense_units=dense_units,\n",
    "            epochs=epochs\n",
    "            )\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(prediction[0])\n",
    "      else:\n",
    "        # do not retrain model, just make forecast\n",
    "        prediction = mlp_forecast(\n",
    "            model=model_fit,\n",
    "            test_x=test_x[i].reshape((1, n_steps_in))\n",
    "        )\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(prediction[0])\n",
    "      # add actual observation to history for the next loop\n",
    "      np.append(history, test_y[i])\n",
    "  return model_fit, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7qR0YE9cBWc"
   },
   "source": [
    "# Repeated Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBcPAT1UcGZv"
   },
   "outputs": [],
   "source": [
    "# repeat evaluation of a config\n",
    "# eg : repeat_evaluate(lambda: run_sarimax(arguments), n_repeats=30)\n",
    "def repeat_evaluate(model_runner, n_repeats=3):\n",
    "  # fit and evaluate the model n_repeats times\n",
    "  results = [model_runner() for _ in range(n_repeats)]\n",
    "  # results is an array of tuple (trained model, predictions, rmse) \n",
    "  # get array of train_models from result\n",
    "  arr_trained_models = column_from_array_of_tuples(results, 0)\n",
    "  # get array of predictions from result\n",
    "  arr_predictions = column_from_array_of_tuples(results, 1)\n",
    "  # get array of rmse from result\n",
    "  arr_rmse = column_from_array_of_tuples(results, 2)\n",
    "  return (arr_trained_models, arr_predictions, arr_rmse)\n",
    "\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_scores(name, scores):\n",
    "\t# print a summary\n",
    "\tscores_m, score_std = np.mean(scores), np.std(scores)\n",
    "\tprint('%s: %.3f RMSE (+/- %.3f)' % (name, scores_m, score_std))\n",
    "\t# box and whisker plot\n",
    "\tpyplot.boxplot(scores)\n",
    "\tpyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxOtylRToBAj"
   },
   "source": [
    "# Runner - KRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deD4PiY60Gxb"
   },
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXlxkECi0Fpv"
   },
   "outputs": [],
   "source": [
    "# Note: Both start_date and end_date get included\n",
    "start_date = '2014-01-01'\n",
    "end_date = '2021-03-01'\n",
    "n_steps_in = 1\n",
    "n_steps_out = 30\n",
    "train_size = 1826\n",
    "\n",
    "# define sarimax order and seasonal order\n",
    "sarimax_order = (1, 1, 0) # p,d,q\n",
    "sarimax_seasonal_order = (1, 0, 1, 7) # P,D,Q,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4Q18lEUu-BI"
   },
   "outputs": [],
   "source": [
    "# Constants for saving model runs\n",
    "model_data_folder_path = './output'\n",
    "os.makedirs(model_data_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iPi-QqQ1krv"
   },
   "source": [
    "## Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3AX8C3goC3q"
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "krsCsvPath = './krs_data.csv'\n",
    "krsImporter = DatasetImporter(krsCsvPath, start_date, end_date)\n",
    "krsImporter.import_data()\n",
    "# get endog and exog arrays\n",
    "endog = krsImporter.arr_endog\n",
    "exog = krsImporter.arr_exog\n",
    "print(f'Shape of endogenous array = {endog.shape}')\n",
    "print(f'Shape of exogenours array = {exog.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvbzpcemKUr4"
   },
   "source": [
    "## Add fourier terms to exogenous array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMA8vgRAKXKV"
   },
   "outputs": [],
   "source": [
    "krs_df_exog = krsImporter.df_exog\n",
    "\n",
    "# construct fourier columns\n",
    "fourier_exog = pd.DataFrame({'date': krsImporter.df_endog.index})\n",
    "fourier_exog = fourier_exog.set_index(pd.PeriodIndex(fourier_exog['date'], freq='D'))\n",
    "fourier_exog['sin365'] = np.sin(2 * np.pi * fourier_exog.index.dayofyear / 365.25)\n",
    "fourier_exog['cos365'] = np.cos(2 * np.pi * fourier_exog.index.dayofyear / 365.25)\n",
    "fourier_exog = fourier_exog.drop(columns=['date'])\n",
    "\n",
    "# set fourier columns for river level exogenous variables\n",
    "krs_df_exog['sin365'] = fourier_exog['sin365'].values\n",
    "krs_df_exog['cos365'] = fourier_exog['cos365'].values\n",
    "\n",
    "exog = krs_df_exog.values\n",
    "print(f'Shape of exogenours array = {exog.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh6aDHSj2KpI"
   },
   "source": [
    "## Run Sarimax Weekly Walk Forward Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUhKwsuEBmUQ"
   },
   "source": [
    "### Run SARIMAX, Calculate and print predictions, error and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Nsuyg542R90"
   },
   "outputs": [],
   "source": [
    "(sarimax_model, \n",
    "sarimax_predictions, \n",
    "sarimax_rmse, \n",
    "sarimax_nse) = sarimax_weekly_walk_forward_validation(\n",
    "    endog, \n",
    "    exog, \n",
    "    sarimax_order, \n",
    "    sarimax_seasonal_order, \n",
    "    n_steps_out, \n",
    "    train_size)\n",
    "print(f\"Predictions shape = {pd.DataFrame(sarimax_predictions).shape}\")\n",
    "print(f\"NSE = {sarimax_nse}\")\n",
    "print(f\"RMSE = {sarimax_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLIRgbodBvB-"
   },
   "source": [
    "### Save SARIMAX trained model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fosDrNK1VbFV"
   },
   "outputs": [],
   "source": [
    "save_predictions(sarimax_predictions, 'sarimax_predictions')\n",
    "save_pickle_model(sarimax_model, 'sarimax_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxT4ShsEBpxc"
   },
   "source": [
    "## Run MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H6JXyyvD2Ay"
   },
   "source": [
    "### Run MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgarBserBm8Z"
   },
   "outputs": [],
   "source": [
    "# calculate train residuals\n",
    "train_residuals = sarimax_model.resid[:train_size-1]\n",
    "\n",
    "# calculate test residuals\n",
    "train, test = train_test_split(endog, train_size=train_size)\n",
    "actuals = test[0: len(test) - n_steps_out].reshape(len(test) - n_steps_out,)\n",
    "first_predictions = np.array(sarimax_predictions)[:,0]\n",
    "test_residuals = calculate_test_residuals(actuals, first_predictions)\n",
    "\n",
    "# run mlp weekly walk forward validation\n",
    "(mlp_model, mlp_predictions) = mlp_weekly_walk_forward_validation(\n",
    "    train=train_residuals, \n",
    "    test=test_residuals, \n",
    "    n_steps_in=n_steps_in, \n",
    "    n_steps_out=n_steps_out\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTduES-kD-DM"
   },
   "source": [
    "### Save MLP Predictions and trained MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHC4Uj6bEA_x"
   },
   "outputs": [],
   "source": [
    "save_predictions(mlp_predictions, 'mlp_predictions')\n",
    "save_NN_model(mlp_model, 'mlp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07Fl-bzACMqA"
   },
   "source": [
    "## Compute final predictions by adding the linear and non linear components\n",
    "\n",
    "***final_prediction = linear (arima/sarimax prediction) + non-linear (NN prediction)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXS6YZQNGM_G"
   },
   "source": [
    "###  Compute final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw8v5oZ-CUGS"
   },
   "outputs": [],
   "source": [
    "final_predictions = np.add(sarimax_predictions[:len(mlp_predictions)], mlp_predictions)\n",
    "print(f'Shape of final predictions = {np.shape(final_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvGxdMDZK00r"
   },
   "source": [
    "### Store final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2rCPRecK327"
   },
   "outputs": [],
   "source": [
    "save_predictions(final_predictions, 'hybrid_model_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1nOZK9CEMtA"
   },
   "source": [
    "## Compute RMSE & NSE of Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uC5uaL4ERsT"
   },
   "outputs": [],
   "source": [
    "# compute actual values\n",
    "actual = step_sequence_converter(test.reshape(len(test),), n_steps_out)[:len(final_predictions)]\n",
    "# compute and print RMSE\n",
    "final_rmse = measure_rmse(actual, final_predictions)\n",
    "print(f\"RMSE of hybrid model = {final_rmse}\")\n",
    "# compute and print NSE\n",
    "final_nse = measure_nse(np.array(actual).flatten(), np.array(final_predictions).flatten())\n",
    "print(f\"NSE of hybrid model = {final_nse}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HybirdModel-SARIMAX+MLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
